---
title: "Milestone Report"
author: "Jerad Acosta"
date: "March 25, 2015"
output: html_document
---

```{r set_options, echo=FALSE, cache=FALSE, warning=FALSE, message=FALSE}
# Remove Scientific Notation 
# and rounded to 2 digits 
options(scipen = 999, digits = 2)
options(width = 80)
library(ggplot2)
library(dplyr)
tab4 <- "    "
tab6 <- "      "
tab8 <- "        "
```
#### Reproducible Work
Just as in any science, reproducibility is important for many reasons   

__Reproducibility:__

* Allows others to check your work and confirm findings
* Proliferates knowledge and techniques   

_and most importantly for us:_   

* Presents insight into the processes which produced our results

To see more of the code used to create this report please see the [GitHub repo](https://github.com/irJERAD/PredictiveTextModeling/blob/master/MilestoneReport/MilestoneReport.Rmd)

## Downloading the Data
The data is from a corpus called [HC Corpora](http://www.corpora.heliohost.org/).
You can see the [readme file](http://www.corpora.heliohost.org/aboutcorpus.html) for more information about the corpora   
To properly allocate the data we will create directories for the original data seperate from what we make of it.  
__If you would like to see the details of this portion please visit the linked [GitHub page](https://github.com/irJERAD/PredictiveTextModeling/blob/master/MilestoneReport/MilestoneReport.Rmd)__
```{r, download_data, eval=FALSE, echo=FALSE}
# create directory for original data
if(!file.exists("./data")) {dir.create("./data")}
# create directory for new, augmented or R created data
if(!file.exists("./data/Rdata")) {dir.create("./data/Rdata")}

Zipurl <- c("https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip")

# because url is to a zipfile we will create a temp file until we can unzip it
temp <- tempfile()
download.file(Zipurl, temp, method = "curl")
# unzip only the en_US files we wish to work with
# final/en_US/en_US.twitter.txt
unzip(temp, files = "final/en_US/en_US.twitter.txt", exdir = "./data")
# final/en_US/en_US.news.txt
unzip(temp, files = "final/en_US/en_US.news.txt", exdir = "./data")
# final/en_US/en_US.blogs.txt
unzip(temp, files = "final/en_US/en_US.blogs.txt", exdir = "./data")
# Unlink temp file to remove file from temporary storage
unlink(temp)
```

---

## Quanitfying the Data
We can capture **summary data** by extracting baisc information from the files.   
Using the R commond `file.info(filePath)$size` for file size, we find:

* The twitter file size is `r file.info("data/final/en_US/en_US.twitter.txt")$size / 2^20`MB
* The News file size is `r file.info("data/final/en_US/en_US.news.txt")$size / 2^20`MB 
* The Blogs file size is `r file.info("data/final/en_US/en_US.blogs.txt")$size / 2^20`MB

Using the **wc** utility with options `-l`,`-w`, and `-m` we capture the line, word and character counts respectively
```{r, engine='bash', count_twitters, cache=TRUE}
wc -lwm data/final/en_US/en_US.twitter.txt
```
* Thus, __the twitter file has:__ 
    + 2360148 lines
    + 30373603 words
    + 166816544 characters  

Employing the same `wc` utility with the `-lwm` options on the rest of the files we get
```{r, engine='bash', count_news-blogs, eval=TRUE, echo=FALSE, cache=TRUE}
wc -lwm data/final/en_US/en_US.news.txt
wc -lwm data/final/en_US/en_US.blogs.txt
```
* __The News files has:__
    + 1010242 lines
    + 34372530 words
    + 205243643 characters
   
* __The Blogs file has:__
    + 899288 lines
    + 37334147 words
    + 208623081 characters

We have highlighted a **basic summary of the of the three files.**   
Now we can arrage this data to obtain some insight into the differences that might identify these sources as well as some similarities which may be underlying facets of the language they are written in.
```{r Line_Word_Char_M, echo=FALSE, cache=TRUE}
LWCgrid <- as.data.frame(matrix(c(2360148,1010242,899288,30373603,34372530,37334147,166816544,205243643,208623081),
       nrow=3, ncol=3, dimnames = list(c("twitter","news","blogs"),
                                       c("Lines","Words","Characters"))))
LWCgrid$'Words per Line' <- LWCgrid$Words / LWCgrid$Lines
LWCgrid$'Char per Word' <- LWCgrid$Characters / LWCgrid$Words

twit2news <- LWCgrid[1,] / LWCgrid[2,]
rownames(twit2news)  <- "twitter/news"
twit2blogs <- LWCgrid[1,] / LWCgrid[3,]
rownames(twit2blogs)  <- "twitter/blogs"
news2blogs <- LWCgrid[2,] / LWCgrid[3,]
rownames(news2blogs)  <- "news/blogs"

LWCgrid  <- rbind(LWCgrid,twit2news, twit2blogs, news2blogs)
LWCgrid <- format(LWCgrid, scientific = FALSE, drop0trailing = TRUE)
LWCgrid
```
*<small>The above table has been created to highlight the similarities and differences in the corpora so that we might begin to glean some information about the files, their contents, and even language</small>*   

******

## Analysis

**Twitter and News Files**  
Where the twitter file has more than twice as many lines and over five times the amount of words as the news file, the news file contains more characters.   
The blogs and news files have similar word, line and character counts.   
With `r as.numeric(LWCgrid[4,5]) * 100`% similarity in word length    

* The compared to the news file, the twitter file has:
    + Slightly over `r as.numeric(LWCgrid[4,1])` times as many lines 
    + Yet only `r as.numeric(LWCgrid[4,3]) * 100`% as many words
    + And `r as.numeric(LWCgrid[4,4]) * 100`% as many words per line
* Not surprisingly, both twitter and news files had a similar number of characters per _word on average_
    + The twitter file had an average of `r as.numeric(LWCgrid[1,5])` characters per word
    + The news file had the largest average of `r as.numeric(LWCgrid[2,5])` character per word

The twitter and news file contrasts makes sense since twitter is composed of tweets which are short, 140 character lines. This could be considered an indentifying trait of a twitter corpus.

**Blog and News Files**   

* The Blogs and News Files were largely similar on a preliminary comparison:
    + The blogs and news files count for Lines, Words and characters were all within 10% of each other
* The largest discrepancy between the two files was the Words per Line count
    + The news file had `r as.numeric(LWCgrid[2,4])` words per line on average
    + The blogs file had an average of `r as.numeric(LWCgrid[3,4])` words per line

`r tab8`While the blogs and news files did not have as large of a deviation from each other as from the the twitter file's average of `r as.numeric(LWCgrid[1,4])` words per line, the blogs file had `r (1 - as.numeric(LWCgrid[6,4])) * 100`% more words per line than the news files. This was the only of these factors these factors that differed by more than 10% between the news and blog files.   

`r tab8`It makes sense that the blogs and news files have more in common with each other than the twitter file since both blogs and the news are made up of articles and not short phrases.    
`r tab8`The largest variance between the two being words per line also makes sense. Blogs are written in more of a narrative voice, often creating run on sentences similar to every day speech. News articles are written to be concise and terminal in nature.

******

### Conclusion on file traits

* __Twitter has `r as.numeric(LWCgrid[4,1])`x more lines than news and `r as.numeric(LWCgrid[5,1])`x more than blogs__ while still maintaining a base of at least `r as.numeric(LWCgrid[5,2]) * 100`% as many words   
    + This is indicative of the nature of twitter
        + twitter is composed of tweets: _140 or less character lines_
    + The News and Blogs, however, consist of articles
        + Articles follow a grammatic methodology to explain
* __All files have `r as.numeric(LWCgrid[1,5])`-`r as.numeric(LWCgrid[2,5])` characters per word__ _on average_   
    + This makes sense as all the text files are in the same language: _English_.
        + Each word is being pulled from the same ultimate repository of the complete list of english words
            + One could even say that at such large volumes the law of large numbers comes into play with the average length of any set of words pulled from the same set, _the collection of English words_, converging on an average similar to the average of all the english words _(weighted by usage or frequency of course)_
* __Blogs have more words per line than news articles__
    + Indicative of the nature and purpose of both blogs and new articles
        + Blogs tend to follow a narrative and write closer to common speach with run on, compound, complex and _longer sentences_
        + The purpose of news is to convey information in an easy to understand and straight forward manner, leading to simpler and _shorter sentences_

******

## Further Analysis

* __Goals__
    + Filter Profanity and unnecessary data
    + Tokenize data
    + Explore Preliminary Statistics
    + Visualize summary statistics

Here we use a profanity list found at [banned word list.com](http://www.bannedwordlist.com)
```{r profanity_filter, eval=FALSE}
badUrl <- c("http://www.bannedwordlist.com/lists/swearWords.txt")
download.file(badUrl, "./data/Rdata/badWords.txt")
```

```{r tokenization_trials, engine='bash', cache=TRUE, eval=FALSE, echo=FALSE}
## Tokenizing using Bash originally
## switched to R for tokenization -- just saving this for referrence
# check twitter with "[:alpha:]" method
# uses -i case insensitive: YOU == You != you
tr -cs "[A-Za-z]" "\n" < data/final/en_US/en_US.twitter.txt | sort -n -r | uniq -ci | cat > ./data/twitAZ.txt
# uses "[:alpha:]"
tr -cs "[:alpha:]" "\n" < data/final/en_US/en_US.twitter.txt | sort -n -r | uniq -ci | cat > ./data/twitAlpha.txt
tr -cs "A-Za-z" "\n" < data/final/en_US/en_US.twitter.txt | sort -r | uniq -c -i | sort -nr | cat > ./data/twitOrder.txt
# convert all caps
# to make The and the become the same word by mapping all uppercase letters to lower case letters
tr 'A-Z' 'a-z' < data/final/en_US/en_US.twitter.txt | tr -sc 'A-Za-z' '\n' | sort -n -r | uniq -ci | cat > ./data/twitNoCap.txt
```


### Tokenization
__Tokenization__ is the task of chopping up a body of text, _or corpus_, into pieces we call __tokens__.   
`r tab4`For example: 
"We went to the mall and had a great time, then we went back home"   
Could be _tokenized_ into: 
|We| |went| |to| |the| |mall| |and| |had| |a| |great| |time| |then| |we| |went| |back| |home|   
_notice the comma was removed from the text_   

* There is no one way to tokenize a corpus and in fact there are many methods all with benefits and weeknesses which can be employed at the discretion of the analyst. How a text is tokenized often depends on what the goal of the analysis is.   
* With a goal of creating a predictive text model, word frequency is one of the most important features of our tokenization process.   
* In the previous example, the token |went| would have an undisputed frequency value of 2. |we|, however could be given a value of 2 if we ignore capitalization or both |we| and |We| could be separately tokenized.   
* Since the tokens will be used in an ngram model the assumption of existing input text can be used to negate the prediction of capitalized First words. 
    + Additionally, capitalized proper nouns should be considered rare enough in this case as do be ignored.     
    
In any case, a robust model will be created which can be edited over time if a larger scope is taken into practice. Acronyms, slang and purposefully misspelled words as well as hashtags for the twitter file come to mind as immediate afterthoughts.

Using the Unix utilities `tr`, `sort`, `uniq` and `cat` along with the pipe command, `|`, we will take our downloaded raw data text files and tokenize them by words

__Tokenization Steps:__

1. Use `tr` to change all capital letters to lowercase letters
    + `tr 'A-Z' 'a-z' < filePath`
2. Create a new line `\n` for each word using `tr` with options `s` and `c`
    + `tr -sc 'A-Za-z' '\n'`
3. Sort the words alphabetically so all similar and same words are next to each other
    + `sort -n -r`
4. Use `uniq` to combine same words while keeping count of how many of each there are with option `c` and disregarding capitalization with option `c`
    + `uniq -ci`
5. Sort again with `sort` using the option `n` to order by count and `r` to reverse order (so that highest frequency words are on top of the list)
    + `sort -nr`
6. Finally, we write the new data into a text file using the `cat` utility
    + `cat > destinationPath.txt`
    
We can easily nest all of these functions with each utility and their associated options by using the pipe command `|`   
The pipe command is as though you told the interpreter to "Make or Do This THEN Make or Do something with what you created or did"   
Where this would be written as `Do This to make That | Do something with That`    
Our final tokenization script will look like this:   
`tr 'A-Z' 'a-z' < filePath | tr -sc 'A-Za-z' '\n' | sort -n -r | uniq -ci | sort -nr | cat > destinationPath`
```{r tokenize, engine='bash', cache=TRUE, eval=FALSE, echo=FALSE}
# Match words, Ignore Case, Order by highest frequency, write to new file
# Twitter set
tr 'A-Z' 'a-z' < data/final/en_US/en_US.twitter.txt | tr -sc 'A-Za-z' '\n' | sort -n -r | uniq -ci | sort -nr | cat > ./data/Rdata/twitter.txt
# news
tr 'A-Z' 'a-z' < data/final/en_US/en_US.news.txt | tr -sc 'A-Za-z' '\n' | sort -n -r | uniq -ci | sort -nr | cat > ./data/Rdata/news.txt
# Blog text
tr 'A-Z' 'a-z' < data/final/en_US/en_US.blogs.txt | tr -sc 'A-Za-z' '\n' | sort -n -r | uniq -ci | sort -nr | cat > ./data/Rdata/blogs.txt
```

__Number of Tokens__ _unique words_   
Now that we are working with smaller file sizes of `r file.info("./data/Rdata/twitter.txt")$size / 2^20`MB, `r file.info("./data/Rdata/news.txt")$size / 2^20`MB, and `r file.info("./data/Rdata/blogs.txt")$size / 2^20`MB for the twitter, news and blogs unique word counts respectively. We can use the R function `read.table(filePath)` to create a data frame the tokens for further inspection.
```{r word_table, cache=TRUE, eval=TRUE, echo=FALSE}
# Create data frame from word count text files
twitterWords <- read.table('./data/Rdata/twitter.txt')
newsWords <- read.table('./data/Rdata/news.txt')
blogsWords <- read.table('./data/Rdata/blogs.txt')

# Name Columns of data frames
names(twitterWords) <- c("Freq","Word")
names(newsWords) <- c("Freq","Word")
names(blogsWords) <- c("Freq","Word")
```

```{r unique_count_vars, cache=TRUE, eval=TRUE, echo=FALSE}
uniqTwitter <- nrow(twitterWords)
uniqNews <- nrow(newsWords)
uniqBlogs <- nrow(blogsWords)
```

* Using `nrow(data.frame)` we find:
    + The twitter file has `r uniqTwitter` unique words
    + The news file has `r uniqNews` unique words
    + The blogs file has `r uniqBlogs` unique words

__Most Common UniGrams__

```{r graph_top_count, cache=TRUE, eval=TRUE, echo=FALSE, fig.height=7, fig.width=9}
twitterWords$Word <- factor(twitterWords$Word, levels = twitterWords$Word[order(twitterWords$Freq, decreasing = FALSE)])
newsWords$Word <- factor(newsWords$Word, levels = newsWords$Word[order(newsWords$Freq, decreasing = FALSE)])
blogsWords$Word <- factor(blogsWords$Word, levels = blogsWords$Word[order(blogsWords$Freq, decreasing = FALSE)])

twit1gram <- ggplot(twitterWords[1:50,], aes(x=Word, y= Freq)) + geom_bar(stat="identity")
twit1gram <- twit1gram + labs(title = "Top 50 Twitter Unigrams", x = "Word", y = "Frequency")
twit1gram  <- twit1gram + coord_flip()

news1gram <- ggplot(newsWords[1:50,], aes(x=Word, y= Freq)) + geom_bar(stat="identity")
news1gram <- news1gram + labs(title = "Top 50 News Unigrams", x = "Word", y = "Frequency")
news1gram  <- news1gram + coord_flip()

blogs1gram <- ggplot(blogsWords[1:50,], aes(x=Word, y= Freq)) + geom_bar(stat="identity")
blogs1gram <- blogs1gram + labs(title = "Top 50 Blogs Unigrams", x = "Word", y = "Frequency")
blogs1gram <- blogs1gram + coord_flip()

twit1gram
news1gram
blogs1gram
```
    
a __unigram__ is a single word phrase, so in this case we are simply listing the highest frequency words from each corpus. Next we will be creating creating statistics from n-gram or multi-word phrases to help in building our prediction model.    


Quickly looking over these exploratory bar charts we can see a few important things:

* There needs to be filtering of certain letters from the token set
    + most likely from compound words like |wasn't| -> |wasn| |t|
* a vast majority of the corpora is composed of Stop-words such as the, and, to, a, of, etc
    + This is difficult because while these statistically skew our data, they are obviously the most commonly typed words and thus important for predictive text modeling
        + ngram modeling should significantly help in indentifying the distribution of these words
    + Being universal across all three files is a valuable insight into what some of the most predicted words will be
    + Also likely says something about the language english itself
    

******

#### Notes

* Each Unix or bash command can be used in the R Console by typing the command in the `system()` function
    + `system("Shell.Command")`
    + example using `wc` utility to count lines of a file located at `filePath`
        + `system("wc -l filePath")` will return the number of lines along with the file path
* Unix and many other shell commands and 
* To pass variables from R to bash use `Sys.setenv(Var = "value")`
    + Then you can use the variable `Var` in a bash code chunk
    + example using extending on previous:
        + In an R code chuck write `Sys.setenv(path = "./data/final/en_US/en_US.twitter.txt")`
        + Then use `path` var in bash code chunk `system("wc -l path")`
            + this is equivalent to writing `wc -l ./data/final/en_US/en_US.twitter.txt` in a bash code chunk
        + makes code and reports easier to read and understand, _particularly in a knitr Rmd file_
        + improves access to variables, outputs from different environments and a more flexible workflow for reusing code
* Knitr can handle almost any language which can be called by command line
    + Examples not limited to: Python, Awk, Ruby, Haskell, Bash, Perl, Graphviz, TikZ, SAS, Scala and CoffeScript
    + To instruct knitr which language is in the code block use the knit_engines objects
        + i.e. {r use-bash, engine = 'bash'}


### Next Steps
>**TODO**   

* Filter unwanted tokens
    + from bad word list
    + letters
    + consider what to do with stop words
* Finish basic n-gram model
    + Make it able to handle unseen n-grams
    + be able to efficiently handle data and storage issues
* Build a Predictive Model on previous data modeling
    + Evaluate the model for efficiency and accuracy
* Explore new models and data to improve predictive model
* Create Shinny App
    + Accepts n-gram and predicts the next word
* Slide Deck
    + Something that you could __pitch__ to an investor

* __Touch Ups__
    + use `Sys.setenv()` to export variables, _like file paths_, to bash code chunks